Rstudio ahora es Posit
Pantalla dividida en 4 partes pero empieza dividida en 3. Consola, script, Environment y directorios
Permite crear proyectos
Importación de datos. En la segunda práctica hay que leer datos de excel. File, import dataset.
Plots para graficos.
Tools Opciones de proyecto y opciones globales
Help. Para informarse.

k-means

En nuestra parte introducir los datos desde un paquete

m<-matrix(c(4,4, 3,5, 1,2, 5,5, 0,1, 2,2, 4,5, 2,1),2,8)
(m<-t(m))

Se va a utilizar un metodo del paquete stats. Hayque elegir el numero k de clusters y los centroides

c<-matrix(c(0,1,2,2),2,2)
(c<-t(c))

Hay que poner el número de iteraciones a realizar en el algoritmo. Habría que ir probando pero sabemos que son 4.
Optimización práctica. Que si no pones iteraciones busca el optimo.

(clasificacionns=(kmeans(m,c,4)))

El clustering vector te dice que elementos pertenecen a cada cluster. Para saber como analizar los clusters.

Hay que separar los clusters. Para ello cbind.

(m=cbind(clasificacionns$cluster, m))

Ahora delante de cada valor aparece a qué cluster pertenece

mc1 = subset(m,m[,1]==1)
mc2 = subset(m,m[,1]==2)

Ahora ya están separados el 1 y el 2. Sobra ya la primera columna.

(mc1=mc1[,-1])
(mc2=mc2[,-1])

En la práctica devolver centroides y los clusters

Clusterizacion Jerárquica Aglomerativa

Instalamos LearnClust con install packages e incluimos con library(x)
Se puede usar ClustLearn en nuestra parte.

Introducimos los datos

m<-matrix(c(0.89, 2.94, 4.36, 5.21, 3.75, 1.12, 6.25, 3.14, 4.1, 1.8, 3.9, 4.27), 2, 6)
> (m<-t(m))

Distintas definiciones de distancia y todas las definiciones de proximidad. 'EUC' distancia euclidea y 'MIN'

agglomerativeHC(m,'EUC', 'MIN')

Los primeros 6 puntos. primera iteracion. Los primeros 6 clusters. Abajo va sacando los clusters de las  iteraciones.

Con details se puede ver que hace exactamente en cada uno de los pasos. Lo explica por cada iteración. Con MAX.

agglomerativeHC.details(m,'EUC', 'MAX')
Se puede con Average, con distancia manhattan, etc.