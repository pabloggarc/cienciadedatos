\documentclass[12pt]{report}\usepackage[]{graphicx}\usepackage[dvipsnames]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[spanish]{babel}
\usepackage[margin = 2.54cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{amssymb, amsthm, array, cancel, enumitem, fancyhdr, float, graphicx, hyperref, hologo, listings, mathtools, qtree, tikz, tikz-cd}
\usepackage[spanish, noabbrev]{cleveref}

\pagestyle{fancy}
\lhead{\footnotesize \leftmark}
\rhead{\footnotesize \rightmark}

\lstdefinestyle{estilo_pablo}{
	basicstyle = \ttfamily\footnotesize, 
	tabsize = 2, 
	commentstyle = \color{gray}, 
	keywordstyle = \color{cyan}, 
	stringstyle = \color{purple}, 
	tabsize = 1, 
	frame = tb, 
	breaklines = true, 
	showstringspaces = false, 
	numbers = left, 
	numberstyle = \footnotesize\color{gray}, 
	stepnumber = 1, 
	captionpos = b
}

\crefname{listing}{Código}{Códigos}
\crefname{section}{Sección}{Secciones}

\title{
	\huge
	\noindent\textbf{Fundamentos de la Ciencia de Datos}\\
	
	{\Large \textit{Práctica 2}}
	\vspace{1cm}
	
	\huge
	Grado en Ingeniería Informática\\
	Universidad de Alcalá\\
	
	\vspace{1cm}
	
	\includegraphics[scale = 0.075]{img/logo}
}

\author{
	Grupo 9\\\\
	Pablo García García\\
	Abel López Martínez\\
	Álvaro Jesús Martínez Parra\\
	Raúl Moratilla Núñez
}

\date{
	\large{\today}
}

\hypersetup{
	pdftitle = {Práctica 2}, 
	pdfauthor = {Pablo García García, Abel López Martínez, Álvaro Jesús Martínez Parra, Raúl Moratilla Núñez}, 
	pdfsubject = {Fundamentos de la Ciencia de Datos}, 
	pdfcenterwindow, 
	pdfnewwindow = true, 
	pdfkeywords = {Entrega de la PL2 de laboratorio correspondiente al Curso 2023-2024}, 
	bookmarksopen = true 
}

\newtheorem{exercise}{Ejercicio}[section]
\newtheorem{observation}{Observación}[section]
\newtcbtheorem[number within = section, Crefname = {Teorema}{Teorema}]{teorema}{Teorema}{fonttitle = \bfseries}{th}
\newtcbtheorem[number within = section, Crefname = {Definición}{Definición}]{definicion}{Definición}{fonttitle = \bfseries}{df}

\newcommand{\dt}{\text{dist}}
\newcommand{\ds}{\text{dens}}
\newcommand{\drm}{\text{drm}}

\newcommand{\rojo}[1]{\textcolor{Red}{#1}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
	
	\renewcommand{\chaptername}{Parte}
	\renewcommand{\lstlistingname}{Código}
	\maketitle \thispagestyle{empty}
	
	\newpage
	
	\setcounter{tocdepth}{3}
	\tableofcontents
	
	\chapter*{Introducción}\addcontentsline{toc}{chapter}{Introducción}\pagestyle{plain}
	
	\section*{RStudio}
	
	RStudio es un entorno de desarrollo integrado (IDE) para R, un lenguaje de programación ampliamente utilizado en estadística, análisis de datos, investigación científica y visualización de datos.
	
	\subsection*{De RStudio a Posit}
	
	El artículo del blog de Posit (\url{https://posit.co/blog/rstudio-is-now-posit/}), anteriormente conocido como RStudio, detalla los cambios y compromisos de la compañía a medida que evoluciona. Fundada en 2009, RStudio se centró en crear software de código abierto de alta calidad para científicos de datos. La compañía ha invertido significativamente en el desarrollo de código abierto, educación y la comunidad, con el objetivo de servir a los creadores de conocimiento durante los próximos 100 años.\\
	
	La transformación de RStudio a Posit no solo implica un cambio de nombre, sino también una reafirmación de sus valores y objetivos. Como una Corporación de Beneficio Público y una B Corp certificada, Posit se compromete a cumplir con los más altos estándares de desempeño social y ambiental, transparencia y responsabilidad. Los directivos de Posit tienen la responsabilidad fiduciaria de abordar las necesidades sociales, económicas y ambientales, además de supervisar los objetivos comerciales de la empresa. La misión de Posit incluye mantener su independencia a largo plazo para cumplir con estos objetivos.\\
	
	El compromiso de Posit con el código abierto sigue siendo una prioridad central. La empresa cree firmemente que el progreso significativo se logra poniendo herramientas de ciencia de datos al alcance de todos, independientemente de sus medios económicos. Además, Posit busca expandir su enfoque más allá del lenguaje de programación R, abrazando también a la comunidad de Python y a otras comunidades de programación. Aunque R sigue siendo un componente clave, el objetivo es mejorar la comunicación científica para todos, creando software tanto de código abierto como comercial para R, Python y más.\\
	
	Finalmente, Posit desea fomentar una comunidad más amplia y diversa. Inspirándose en la comunidad de R, la compañía aspira a ayudar a que otras áreas de la ciencia sean tan abiertas, dinámicas, inclusivas y diversas como la comunidad a la que pertenecen. A pesar del cambio de nombre, la dedicación de Posit a ayudar a los científicos de datos a utilizar software de código abierto para plantear y responder preguntas importantes permanece inalterada.
	
	\subsection*{Posit}
	
	La página web de Posit (\url{https://posit.co/}) ofrece una amplia gama de productos y soluciones para la ciencia de datos, enfocándose en el uso de R y Python. Presenta opciones de software de código abierto, soluciones empresariales y basadas en la nube. Destaca su compromiso con la ciencia de datos accesible para todos, ofreciendo recursos educativos, historias de clientes y soporte para la comunidad. La empresa busca promover un entorno inclusivo y empoderador, destacando su papel en el avance de la ciencia de datos a través de la colaboración y la innovación tecnológica.
	
	\subsection*{Entorno RStudio}
	
	La interfaz del IDE de RStudio está diseñada para facilitar el análisis estadístico y la visualización de datos.
	
	\begin{itemize}
	
	\item \textbf{File}: Aquí puedes crear nuevos archivos y proyectos, incluidos scripts de Python y documentos Sweave. Trabajar con proyectos es una práctica profesional estándar y permite una mejor organización.
	
	\item \textbf{Edit}: Esta sección funciona de manera similar a otros entornos de desarrollo, proporcionando herramientas de edición de texto estándar.
	
	\item \textbf{Code}: Incluye herramientas específicas para escribir y ejecutar código R, como insertar operadores o ejecutar scripts completos.
	
	\item \textbf{View}: Permite ajustar la apariencia de la interfaz de RStudio, como mostrar u ocultar diferentes paneles.
	
	\item \textbf{Plots}: Muy utilizado para la visualización de datos, permite generar, ver y manipular gráficos estadísticos.
	
	\item \textbf{Session}: Controla las sesiones de R, permitiendo iniciar, interrumpir o terminar sesiones.
	
	\item \textbf{Build}: Herramientas relacionadas con la construcción de paquetes de R.
	
	\item \textbf{Debug}: Funciones para depurar el código R.
	
	\item \textbf{Profile}: Herramientas para medir el rendimiento del código R.
	
	\item \textbf{Tools}: Opciones para configurar el entorno de desarrollo globalmente, como la disposición del panel y opciones de proyecto.
	
	\item \textbf{Help}: Ofrece documentación y ayuda para usar R y RStudio, un recurso esencial para el aprendizaje y la solución de problemas.
	
	\end{itemize}
		
	El entorno de RStudio es altamente personalizable y se organiza en cuatro áreas principales. Puedes redactar código en el panel de \texttt{scripts}, ejecutar instrucciones en el \texttt{console}, y ver los objetos activos y el historial en el área de \texttt{environment/history}. El cuarto panel es multifuncional, permitiendo administrar archivos, visualizar datos a través de gráficos, instalar y administrar paquetes y buscar en la documentación en la sección \texttt{files/plots/packages/help/viewer}. Este diseño modular facilita la gestión eficiente del flujo de trabajo en análisis de datos y desarrollo estadístico.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale = 0.15]{img/posit}
		\caption{Logo de la compañía Posit}
		\label{fig:logo_posit}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale = 0.15]{img/rstudio}
		\caption{Logo del entorno RStudio}
		\label{fig:logo_rstudio}
	\end{figure}
	
	\chapter{Ejercicios guiados}\pagestyle{fancy}
	
	En esta primera parte de la práctica, se repetirán los ejercicios explicados y realizados por el profesor en las clases de laboratorio, utilizando los mismos procedimientos vistos y plasmándolos en este documento.
	
	\section{Clasificación No Supervisada con K-Means}
	
	\begin{exercise}
		El primer conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con KMeans, estará formado por las siguientes 8 calificaciones de estudiantes: 1. \{4, 4\}; 2. \{3, 5\}; 3. \{1, 2\}; 4. \{5, 5\}; 5.\{0, 1\}; 6. \{2, 2\}; 7. \{4, 5\}; 8. \{2, 1\}, donde las características de las calificaciones son: \{Teoría, Laboratorio\}.
	\end{exercise}
	
	En este ejercicio se va a detallar cómo realizar una clasificación no supervisada en un conjunto de datos en R; concretamente utilizando el algoritmo K-Means. El objetivo será agrupar subconjuntos de los datos en clusters o grupos, creando de esta forma una clasificación del conjunto total. Estos clusters se determinarán en base a la muestra, obteniéndose durante el mismo proceso de clasificación.\\
	
	El algoritmo K-Means ofrece una técnica de clusterización en base a una muestra de datos y una cantidad $n$ de clusters. Para empezar a realizar el algoritmo se necesita la ubicación de los centroides de cada cluster. Un centroide es el punto medio del grupo de sucesos que componen el cluster, por lo que habrá tantos centroides como $n$ clusters haya. La cantidad de clusters así como sus centroides iniciales serán elegidos arbitrariamente por el usuario; estos se irán reubicando a medida que se itere en el algoritmo. \\
	
	Para empezar, en R se necesita introducir la muestra o conjunto de datos con el que se va a trabajar. Estos datos serán introducidos en una matriz utilizando la función \texttt{matrix}.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m}\hlkwb{<-}\hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlnum{4}\hlstd{,} \hlnum{3}\hlstd{,}\hlnum{5}\hlstd{,} \hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,} \hlnum{5}\hlstd{,}\hlnum{5}\hlstd{,} \hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,}\hlnum{2}\hlstd{,} \hlnum{4}\hlstd{,}\hlnum{5}\hlstd{,} \hlnum{2}\hlstd{,}\hlnum{1}\hlstd{),}\hlnum{2}\hlstd{,}\hlnum{8}\hlstd{)}
\hlstd{(m}\hlkwb{<-}\hlkwd{t}\hlstd{(m))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    4    4
## [2,]    3    5
## [3,]    1    2
## [4,]    5    5
## [5,]    0    1
## [6,]    2    2
## [7,]    4    5
## [8,]    2    1
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Para poder trabajar debidamente se debe trasponer la matriz, de tal forma que cada columna represente una componente. El primer punto conformará la primera fila, el segundo punto la segunda fila y así sucesivamente con todos los datos.\\
	
	Además del conjunto de datos, el algoritmo K-Means necesita unos centroides iniciales. Estos se introducirán de la misma forma que la muestra, teniendo en la primera fila el centroide del primer cluster, en la siguiente fila el centroide del segundo... Al tratarse de pocos puntos se eligen arbitrariamente dos centroides, por lo que la clasificación final será realizada con dos clusters. Se introducen así los centroides:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{c}\hlkwb{<-}\hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{),}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{)}
\hlstd{(c}\hlkwb{<-}\hlkwd{t}\hlstd{(c))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    0    1
## [2,]    2    2
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Con la muestra y los centroides introducidos en el entorno de trabajo, ya se puede realizar el algoritmo. Para ello se utilizará la función \texttt{kmeans}, función del paquete cargado por defecto \texttt{stats}. La función recibe tres parámetros: la muestra de los datos (\texttt{m}), los centroides iniciales de los clusters (\texttt{c}) y el número de iteraciones que se desean. En este caso se eligen cuatro iteraciones, las mismas que se necesitaron en clase de teoría. En teoría, el número de iteraciones no se sabe a priori, por lo que habría que ir probando. Sin embargo, como ya se sabe el número que se necesita, se pone directamente 4.
	
	{\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(clasificacionns} \hlkwb{=} \hlstd{(}\hlkwd{kmeans}\hlstd{(m,c,}\hlnum{4}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## K-means clustering with 2 clusters of sizes 4, 4
## 
## Cluster means:
##   [,1] [,2]
## 1 1.25 1.50
## 2 4.00 4.75
## 
## Clustering vector:
## [1] 2 2 1 2 1 1 2 1
## 
## Within cluster sum of squares by cluster:
## [1] 3.75 2.75
##  (between_SS / total_SS =  84.8 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"
\end{verbatim}
\end{kframe}
\end{knitrout}
	}
	
	Entre los parámetros que aparecen en la salida se encuentran \texttt{Cluster means} y \texttt{Clustering vector}. El primero de ellos proporciona la ubicación de los dos centroides de los clusters que conforman la clasificación final. Como se puede apreciar, salen los dos centroides que salieron en teoría. El segundo parámetro indica a qué cluster pertenece cada suceso de la muestra introducida. Así el punto 1 pertenece al cluster 2, el punto 2 pertenece al cluster 2, el punto 3 al cluster 1... Este último se puede utilizar como entrada para realizar otras tareas. Puede ser muy útil para analizar cada cluster por separado, ver sus caracteristicas comunes e intentar deducir por qué esos datos están relacionados.\\
	
	El siguiente comando permite añadir una columna a la izquierda de la muestra de datos (\texttt{m}) que se había introducido al inicio, indicando a qué cluster pertenece cada suceso.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(m}\hlkwb{=}\hlkwd{cbind}\hlstd{(clasificacionns}\hlopt{$}\hlstd{cluster,m))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    2    4    4
## [2,]    2    3    5
## [3,]    1    1    2
## [4,]    2    5    5
## [5,]    1    0    1
## [6,]    1    2    2
## [7,]    2    4    5
## [8,]    1    2    1
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Esto se realiza para poder dividir la muestra según la clasificación que se ha conseguido. Llamando a la función subset se puede extraer la porción de la muestra que pertenece al cluster 1 y la porción restante que pertenece al cluster 2.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(mc1}\hlkwb{=}\hlkwd{subset}\hlstd{(m,m[,}\hlnum{1}\hlstd{]}\hlopt{==}\hlnum{1}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    1    2
## [2,]    1    0    1
## [3,]    1    2    2
## [4,]    1    2    1
\end{verbatim}
\begin{alltt}
\hlstd{(mc2}\hlkwb{=}\hlkwd{subset}\hlstd{(m,m[,}\hlnum{1}\hlstd{]}\hlopt{==}\hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    2    4    4
## [2,]    2    3    5
## [3,]    2    5    5
## [4,]    2    4    5
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Una vez se tienen los datos separados se puede eliminar la columna que indica el cluster al que pertenece cada dato, ya que se sobreentiende que todos pertenecen al mismo cluster porque ya han pasado por un proceso de separación en función de la clasificación. Por ejemplo, para los datos pertenecientes al primer cluster, se haría de la siguiente forma:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(mc1} \hlkwb{=} \hlstd{mc1[,}\hlopt{-}\hlnum{1}\hlstd{])}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    1    2
## [2,]    0    1
## [3,]    2    2
## [4,]    2    1
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Con todo esto se consigue, partiendo de una muestra de datos, una clasificación no supervisada utilizando la técnica de clusterización basada en el algoritmo K-Means. A partir de ella se podrán intentar deducir ciertas conclusiones.
	
	\section{Clasificación No Supervisada con CJA}
	
	\begin{exercise}
		El segundo conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con Clusterización Jerárquica Aglomerativa, estará formado por 6 calificaciones de estudiantes: 1. \{0.89, 2.94\}; 2.\{4.36, 5.21\}; 3. \{3.75, 1.12\}; 4. \{6.25, 3.14\}; 5. \{4.1, 1.8\}; 6. \{3.9, 4.27\}.
	\end{exercise}
	
	La Clusterización Jerárquica Aglomerativa es un método de análisis de datos que comienza tratando cada punto como un cluster individual y luego, iterativamente, combina los clusters más cercanos hasta formar un único cluster. Este método es útil para identificar grupos naturales en los datos. En clase se implementó mediante el paquete \texttt{LearnClust} del CRAN.\\
	
	Inicialmente, se crea una matriz con la muestra del problema, y se transpone la matriz para que tenga el formato deseado.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}
        \hlkwd{c}\hlstd{(}\hlnum{0.89}\hlstd{,}\hlnum{2.94}\hlstd{,} \hlnum{4.36}\hlstd{,}\hlnum{5.21}\hlstd{,} \hlnum{3.75}\hlstd{,}\hlnum{1.12}\hlstd{,} \hlnum{6.25}\hlstd{,}\hlnum{3.14}\hlstd{,} \hlnum{4.1}\hlstd{,}\hlnum{1.8}\hlstd{,} \hlnum{3.9}\hlstd{,}\hlnum{4.27}\hlstd{),}\hlnum{2}\hlstd{,}\hlnum{6}\hlstd{)}
\hlstd{(m} \hlkwb{<-} \hlkwd{t}\hlstd{(m))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,] 0.89 2.94
## [2,] 4.36 5.21
## [3,] 3.75 1.12
## [4,] 6.25 3.14
## [5,] 4.10 1.80
## [6,] 3.90 4.27
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Para la ejecución del algoritmo utilizamos la función \texttt{agglomerativeHC}. Esta función recibe como parámetros la matriz de datos, la métrica de distancia y el criterio de enlace.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{agglomerativeHC}\hlstd{(m,} \hlstr{'EUC'}\hlstd{,} \hlstr{'MIN'}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-8-1} 
\begin{kframe}\begin{verbatim}
## $dendrogram
## Number of objects: 6 
## 
## 
## $clusters
## $clusters[[1]]
##     X1   X2
## 1 0.89 2.94
## 
## $clusters[[2]]
##     X1   X2
## 1 4.36 5.21
## 
## $clusters[[3]]
##     X1   X2
## 1 3.75 1.12
## 
## $clusters[[4]]
##     X1   X2
## 1 6.25 3.14
## 
## $clusters[[5]]
##    X1  X2
## 1 4.1 1.8
## 
## $clusters[[6]]
##    X1   X2
## 1 3.9 4.27
## 
## $clusters[[7]]
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
## 
## $clusters[[8]]
##     X1   X2
## 1 4.36 5.21
## 2 3.90 4.27
## 
## $clusters[[9]]
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
## 3 4.36 5.21
## 4 3.90 4.27
## 
## $clusters[[10]]
##     X1   X2
## 1 6.25 3.14
## 2 3.75 1.12
## 3 4.10 1.80
## 4 4.36 5.21
## 5 3.90 4.27
## 
## $clusters[[11]]
##     X1   X2
## 1 0.89 2.94
## 2 6.25 3.14
## 3 3.75 1.12
## 4 4.10 1.80
## 5 4.36 5.21
## 6 3.90 4.27
## 
## 
## $groupedClusters
##   cluster1 cluster2
## 1        3        5
## 2        2        6
## 3        7        8
## 4        4        9
## 5        1       10
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	En este código, `EUC' representa la métrica de distancia euclidiana y `MIN' el criterio para la agrupación de clusters. En la salida se pueden observar cuatro apartados.\\
	
	El primero es una figura los datos de la matriz que hemos introducido impresos en una gráfica.\\
	
	El segundo es el número de puntos que se han introducido, mediante el atributo \texttt{\$dendrogram}, en este caso 6.\\
	
	El tercer apartado es una lista que muestra las coordenadas de todos los clusters al finalizar la ejecución, los 6 primeros son los puntos introducidos y los 5 siguientes son los formados por cada una de las iteraciones uniendo dos clusters anteriores hasta tener todos unidos en el mismo cluster, momento en el que finaliza el algoritmo. Se puede ver en el atributo \texttt{\$clusters}.\\
	
	El cuarto apartado es el proceso que se realiza en cada iteración, se pueden ver en el apartado \texttt{\$groupedClusters}.
	
	\begin{enumerate}
		\item Se unen los clusters 3 y 5, para formar el cluster 7.
		\item Se unen los clusters 2 y 6, para formar el cluster 8.
		\item Se unen los clusters 4 y 8, para formar el cluster 9.
		\item Se unen los clusters 7 y 9, para formar el cluster 10.
		\item Se unen los clusters 1 y 10, para formar el cluster 11.
	\end{enumerate}
	
	Para obtener una explicación detallada paso a paso tal y como se ha explicado en clase, usamos la función \texttt{agglomerativeHC.details}.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{agglomerativeHC.details}\hlstd{(m,} \hlstr{'EUC'}\hlstd{,} \hlstr{'MIN'}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ Agglomerative hierarchical clustering is a classification technique that initializes \\\#\# \ a cluster for each data.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ \ It calculates the distance between datas depending on the approach type given and}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ it creates a new cluster joining the most similar clusters until getting only one.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ 'toList' creates a list initializing datas by creating clusters with each one}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ \ These are the clusters with only one element:}}\begin{verbatim}
## [[1]]
##      [,1] [,2] [,3]
## [1,] 0.89 2.94    1
## 
## [[2]]
##      [,1] [,2] [,3]
## [1,] 4.36 5.21    1
## 
## [[3]]
##      [,1] [,2] [,3]
## [1,] 3.75 1.12    1
## 
## [[4]]
##      [,1] [,2] [,3]
## [1,] 6.25 3.14    1
## 
## [[5]]
##      [,1] [,2] [,3]
## [1,]  4.1  1.8    1
## 
## [[6]]
##      [,1] [,2] [,3]
## [1,]  3.9 4.27    1
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ \ In each step: \\\#\# \ \ \ - It calculates a matrix distance between active clusters depending on the approach and distance type.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ - It gets the minimum distance value from the matrix.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ - It creates a new cluster joining the minimum distance clusters.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ - It repeats these steps while final clusters do not include all datas.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 1}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##          [,1]     [,2]      [,3]     [,4]      [,5]     [,6]
## [1,] 0.000000 4.146541 3.3899853 5.363730 3.4064204 3.290745
## [2,] 4.146541 0.000000 4.1352388 2.803034 3.4198977 1.046518
## [3,] 3.389985 4.135239 0.0000000 3.214094 0.7647876 3.153569
## [4,] 5.363730 2.803034 3.2140940 0.000000 2.5333969 2.607566
## [5,] 3.406420 3.419898 0.7647876 2.533397 0.0000000 2.478084
## [6,] 3.290745 1.046518 3.1535694 2.607566 2.4780839 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 0.764787552199956}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 3, 5}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 3 and cluster 5, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 2}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##          [,1]     [,2] [,3]     [,4] [,5]     [,6]     [,7]
## [1,] 0.000000 4.146541    0 5.363730    0 3.290745 3.389985
## [2,] 4.146541 0.000000    0 2.803034    0 1.046518 3.419898
## [3,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
## [4,] 5.363730 2.803034    0 0.000000    0 2.607566 2.533397
## [5,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
## [6,] 3.290745 1.046518    0 2.607566    0 0.000000 2.478084
## [7,] 3.389985 3.419898    0 2.533397    0 2.478084 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 1.04651803615609}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 2, 6}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 2 and cluster 6, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 4.36 5.21
## 2 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 3}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##          [,1] [,2] [,3]     [,4] [,5] [,6]     [,7]     [,8]
## [1,] 0.000000    0    0 5.363730    0    0 3.389985 3.290745
## [2,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [3,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [4,] 5.363730    0    0 0.000000    0    0 2.533397 2.607566
## [5,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [6,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [7,] 3.389985    0    0 2.533397    0    0 0.000000 2.478084
## [8,] 3.290745    0    0 2.607566    0    0 2.478084 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 2.47808393723861}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 7, 8}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 7 and cluster 8, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
## 3 4.36 5.21
## 4 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 4}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##           [,1] [,2] [,3]     [,4] [,5] [,6] [,7] [,8]     [,9]
##  [1,] 0.000000    0    0 5.363730    0    0    0    0 3.290745
##  [2,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [3,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [4,] 5.363730    0    0 0.000000    0    0    0    0 2.533397
##  [5,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [6,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [7,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [8,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [9,] 3.290745    0    0 2.533397    0    0    0    0 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 2.53339692902632}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 4, 9}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 4 and cluster 9, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 6.25 3.14
## 2 3.75 1.12
## 3 4.10 1.80
## 4 4.36 5.21
## 5 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 5}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##           [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]    [,10]
##  [1,] 0.000000    0    0    0    0    0    0    0    0 3.290745
##  [2,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [3,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [4,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [5,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [6,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [7,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [8,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [9,] 0.000000    0    0    0    0    0    0    0    0 0.000000
## [10,] 3.290745    0    0    0    0    0    0    0    0 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 3.29074459659208}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 1, 10}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 1 and cluster 10, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 0.89 2.94
## 2 6.25 3.14
## 3 3.75 1.12
## 4 4.10 1.80
## 5 4.36 5.21
## 6 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ This loop has been repeated until the last cluster contained every single clusters.}}\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-9-1} 
\end{knitrout}
	
	La función también se puede ejecutar con diferentes criterios de unión, como `MAX' y `AVG', para observar cómo cambian los clusters:\\
	
	\texttt{agglomerativeHC.details(m, `EUC', `MAX')}\\
	
	\texttt{agglomerativeHC.details(m, `EUC', `AVG')}
	
	\section{Clasificación Supervisada con Árboles de Decisión}
	
	\begin{exercise}
		El tercer conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando árboles de decisión, estará formado por las siguientes 9 calificaciones de estudiantes: 1. \{A, A, B, Ap\}; 2. \{A, B, D, Ss\}; 3. \{D, D, C, Ss\}; 4. \{D, D, A, Ss\}; 5. \{B, C, B, Ss\}; 6. \{C, B, B, Ap\}; 7. \{B, B, A, Ap\}; 8. \{C, D, C, Ss\}; 9. \{B, A, C,
		Ss\}, donde las características de las calificaciones son: \{Teoría, Laboratorio, Prácticas, Calificación Global).
	\end{exercise}
	
	El primer paso a ejecutar para poder realizar la clasificación supervisada con árboles de decisión es introducir los datos en un archivo de texto (\texttt{.txt}) que será posteriormente leído con el lenguaje R. Para ello, es necesario que el archivo de texto cumpla con un formato en concreto:
	
	\begin{itemize}
		\item La primera fila contendrá los nombres de las columnas, iniciando la fila con un tabulado o espacio e introduciendo otro entre cada una de las columnas.
		\item La primera columna contendrá los nombres de cada una de las filas del dataframe.
		\item Cada elemento de una fila estará separado del resto con un tabulado o espacio.
		\item El archivo de texto debe finalizar con una fila vacía.
	\end{itemize}
	
	Este archivo de texto se llamará \texttt{aprobados.txt} y será guardado en la carpeta \texttt{data}.
	
	El archivo es cargado en R de la siguiente forma:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(calificaciones}\hlkwb{=}\hlkwd{read.table}\hlstd{(}\hlstr{"data/aprobados.txt"}\hlstd{))}
\end{alltt}
\begin{verbatim}
##    T L P CG
## 1. A A B Ap
## 2. A B D Ss
## 3. D D C Ss
## 4. D D A Ss
## 5. B C B Ss
## 6. C B B Ap
## 7. B B A Ap
## 8. C D C Ss
## 9. B A C Ss
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Para la implementación del algoritmo, se emplea el paquete \texttt{rpart}, el cual se encuentra disponible en el repositorio \texttt{CRAN}. Este paquete ofrece un método homónimo que recibe por parámetro, entre otros, los datos en forma de dataframe. Por ello, es necesario poseer los datos en esta forma en lugar de en una tabla:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(rpart)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning: package 'rpart' was built under R version 4.3.2}}\begin{alltt}
\hlstd{(muestra}\hlkwb{=}\hlkwd{data.frame}\hlstd{(calificaciones))}
\end{alltt}
\begin{verbatim}
##    T L P CG
## 1. A A B Ap
## 2. A B D Ss
## 3. D D C Ss
## 4. D D A Ss
## 5. B C B Ss
## 6. C B B Ap
## 7. B B A Ap
## 8. C D C Ss
## 9. B A C Ss
\end{verbatim}
\begin{alltt}
\hlstd{clasificacion} \hlkwb{=} \hlkwd{rpart}\hlstd{(CG}\hlopt{~}\hlstd{T}\hlopt{+}\hlstd{L}\hlopt{+}\hlstd{P,} \hlkwc{data}\hlstd{=muestra,} \hlkwc{method}\hlstd{=}\hlstr{"class"}\hlstd{,} \hlkwc{minsplit}\hlstd{=}\hlnum{1}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
	
	El primer argumento que se le pasa es \texttt{CG~T+L+P}, que contiene como primer elemento (a la izquierda de \texttt{~}) el criterio que se desea clasificar, y a la derecha de la virgulilla, aparecen las columnas sobre las que se desea clasificar. También es posible poner un punto (\texttt{.}) a la derecha, lo cual significa que se clasificaría en base a todas las columnas que no fueran el criterio de clasificación. El segundo parámetro es, como ya se ha mencionado, la muestra en un dataframe, el tercer parámetro \texttt{ method="class"} indica que se está abordando un problema de clasificación, y el último parámetro \texttt{minsplit=1} es necesario ya que se está trabajando sobre un conjunto de datos pequeño. 
	
	La salida de la función es la siguiente:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{clasificacion}
\end{alltt}
\begin{verbatim}
## n= 9 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 9 3 Ss (0.3333333 0.6666667)  
##   2) L=A,B 5 2 Ap (0.6000000 0.4000000)  
##     4) P=A,B 3 0 Ap (1.0000000 0.0000000) *
##     5) P=C,D 2 0 Ss (0.0000000 1.0000000) *
##   3) L=C,D 4 0 Ss (0.0000000 1.0000000) *
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Se puede apreciar que el árbol consta de dos niveles además de la raíz. El primero de ellos clasifica según la nota en laboratorio, en caso de ser \texttt{C} o \texttt{D}, la calificación final es \texttt{Ss}, en caso de ser \texttt{A} o \texttt{B} es necesario observar el siguiente nivel. Este segundo nivel clasifica según la nota de prácticas, si la nota es \texttt{A} o \texttt{B}, la calificación es \texttt{Ap} y si es \texttt{C} o \texttt{D}, la calificación final es \texttt{Ss}.
		
	\newpage
	
	\section{Clasificación Supervisada con Regresión}
	
	\begin{exercise}
		El cuarto conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando regresión, estará formado por los siguientes 4 radios ecuatoriales y densidades de los planetas interiores:
		\{Mercurio, 2.4, 5.4; Venus, 6.1, 5.2; Tierra, 6.4, 5.5; Marte, 3.4, 3.9\}.
	\end{exercise}
	
	Para empezar este ejercicio se deberán introducir los datos expuestos en el enunciado en un archivo de texto (\texttt{.txt}), con el fin de ser posteriormente leídos. La inserción de los datos en este fichero se hará atendiendo a las normas relacionadas con este tipo de archivos que ya se vieron en la primera práctica. Estas normas son las siguientes:
	
	\begin{itemize}
		\item Existirá una tabulación entre dato y dato. 
		\item La primera columna numera las filas, y en la primera fila se introduce un espacio y el nombre de las variables. 
		\item Se introducirá un salto de línea en la última fila. 
		\item Para los números decimales se utilizarán puntos. 
		\item Al escribir nombres, no se deberán introducir espacios. 
	\end{itemize}
	
	Obedeciendo estas normas, se copian los datos en un fichero llamado \texttt{planetas.txt}, y se carga en R de la siguiente manera:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(muestra} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"data/planetas.txt"}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      R   D
## 1. 2.4 5.4
## 2. 6.1 5.2
## 3. 6.4 5.5
## 4. 3.4 3.9
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Una vez se tienen los datos en R, se procede a hacer uso de la función \texttt{lm} contenida en los paquetes básicos (concretamente en el paquete \texttt{stats}). Esta función recibe dos parámetros. El primero de ellos es la fórmula en donde se va a decir en función de qué parámetro se quiere otro parámetro. En este caso se tiene la columna \texttt{R} que representa el radio y la columna \texttt{D} que representa la densidad. Como se quiere la densidad en función del radio, el primer parámetro tendrá que ser formula=D$\sim$R. Se indica de esta forma que se pretende obtener la columna \texttt{D} en función de la columna \texttt{R}; o lo que es lo mismo, la densidad en función del radio.\\
	
	El segundo parámetro que entra a la función es la estructura que contiene los datos que se pretenden estudiar. En este caso, el parámetro \texttt{data} será la variable \texttt{muestra}, confeccionada previamente.\\
	
	Con los parámetros de la función \texttt{lm} claros, se invoca a la misma de la siguiente forma:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(regresion}\hlkwb{=}\hlkwd{lm}\hlstd{(D}\hlopt{~}\hlstd{R,} \hlkwc{data}\hlstd{=muestra))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = D ~ R, data = muestra)
## 
## Coefficients:
## (Intercept)            R  
##      4.3624       0.1394
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	En la salida de la función se observan los coeficientes que conforman la recta de regresión que mejor se adapta a los datos introducidos. El método de obtención o ajuste de la función es el de mínimos cuadrados, pudiendo comprobar que el resultado es el mismo que el que se ha visto en clase. En este método, los coeficientes se calculan de la siguiente forma, siendo $x$ el radio e $y$ la densidad:
	
	\begin{center}
		$b = \frac{s_{xy}}{s^{2}_x}$;
		$a = \overline{y} -b\overline{x}$
	\end{center}
	 
	 La salida proporciona directamente los valores de a y b, siendo estos el primero y el segundo respectivamente. Con estos valores se puede decir que la densidad de un planeta se puede sacar atendiendo a la siguiente fórmula:
	 
	 \begin{center}
	 	$D = 4.3624 + 0.1394R$
	 \end{center}
	 
	 Con el comando \texttt{summary} aplicado a la regresión que se acaba de hacer se pueden ver parámetros que detallan esta recta de regresión con respecto a los datos.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(}\hlkwd{summary}\hlstd{(regresion))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = D ~ R, data = muestra)
## 
## Residuals:
##       1.       2.       3.       4. 
##  0.70312 -0.01253  0.24566 -0.93624 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)  
## (Intercept)   4.3624     1.2050   3.620   0.0685 .
## R             0.1394     0.2466   0.565   0.6289  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.846 on 2 degrees of freedom
## Multiple R-squared:  0.1377,	Adjusted R-squared:  -0.2935 
## F-statistic: 0.3193 on 1 and 2 DF,  p-value: 0.6289
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Entre todos los detalles que aparecen, se observa el parámetro \texttt{Residuals}. Este parámetro devuelve un vector con los residuos, o dicho de otra forma, la distancia entre el valor real y el valor que se obtiene por medio de la recta calculada. También se puede apreciar el parámetro \texttt{Multiple R-squared}, el cual sirve como medida de cuán bueno ha sido el ajuste. Este valor podrá tomar valores entre 0 y 1, siendo 0 un ajuste muy malo y 1 un ajuste perfecto. Como se observa, el valor de este parámetro es 0.1377, lo cual quiere decir que el ajuste es malo; y con ello se puede concluir que el radio no explica la densidad de los planetas.\\
	 
	 Una vez visto en detalle el cálculo y valoración de la recta de regresión se va a ver cómo se pueden identificar sucesos anómalos mediante la técnica de regresión. El proceso por lo general sigue 5 pasos:
	 
	 \begin{enumerate}
	 	\item Determinar el grado de outlier \texttt{d}
	 	\item Obtener la ecuación de la recta de regresión
	 	\item Obtener el error estándar $s_r$ del vector de residuos
	 	\item Calcular el límite para los valores típicos como $lim = d \cdot s_r$
	 	\item Identificar como outliers los residuos (en valor absoluto) que superen ese límite
	 \end{enumerate}
	 
	 Hasta ahora se tiene la recta de regresión y el vector de residuos que está en \texttt{summary}. Para extraerlo y poder operar con él se hace de la siguiente forma:
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(res}\hlkwb{=}\hlkwd{summary}\hlstd{(regresion)}\hlopt{$}\hlstd{residuals)}
\end{alltt}
\begin{verbatim}
##          1.          2.          3.          4. 
##  0.70312301 -0.01253452  0.24565541 -0.93624389
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Una vez se tiene el vector de residuos se calcula el error estándar del mismo:
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(sr} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(res}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlnum{4}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 0.5982136
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Con ello se puede plantear un bucle el cual compruebe qué elemento o elementos se presentan como anomalías. Se va a elegir como grado de outlier $d = 3$
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(res))\{}
                 \hlkwa{if}\hlstd{(res[i]}\hlopt{>}\hlnum{3}\hlopt{*}\hlstd{sr)\{}
                        \hlkwd{print}\hlstd{(}\hlstr{"el suceso"}\hlstd{);}
                        \hlkwd{print}\hlstd{(res[i]);}
                        \hlkwd{print}\hlstd{(}\hlstr{"es un suceso anómalo o outlier"}\hlstd{)}
                 \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	 
	 Este bucle comprueba por cada elemento del vector de residuos si supera o no el límite establecido para outliers; en caso afirmativo lo imprime por pantalla. En la salida anterior se puede observar que no hay ningún outlier, ya que ningún residuo supera el umbral establecido.\\ 
	 
	 Se va a probar con otro conjunto de datos para demostrar que por medio de este método podemos identificar las anomalías. En primer lugar, se vuelve a confeccionar un archivo de texto (\texttt{.txt}) atendiendo a las normas previamente expuestas. Este archivo de texto se llamará \texttt{planetas2.txt} para evitar problemas de sobrescritura. 
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(muestra} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"data/planetas2.txt"}\hlstd{))}
\end{alltt}
\begin{verbatim}
##       R    D
## 1.  3.0  2.0
## 2.  3.5 12.0
## 3.  4.7  4.1
## 4.  5.2  4.9
## 5.  7.1  6.1
## 6.  6.2  5.2
## 7. 14.0  5.3
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Una vez hecho esto se invoca a la función \texttt{lm} para sacar la regresión y se guarda el vector de residuos tal y como se ha hecho previamente.
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(dfr}\hlkwb{=}\hlkwd{lm}\hlstd{(D}\hlopt{~}\hlstd{R,} \hlkwc{data}\hlstd{=muestra))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = D ~ R, data = muestra)
## 
## Coefficients:
## (Intercept)            R  
##     6.01445     -0.05723
\end{verbatim}
\begin{alltt}
\hlstd{(res}\hlkwb{=}\hlkwd{summary}\hlstd{(dfr)}\hlopt{$}\hlstd{residuals)}
\end{alltt}
\begin{verbatim}
##         1.         2.         3.         4.         5.         6.         7. 
## -3.8427477  6.1858698 -1.6454482 -0.8168308  0.4919157 -0.4595958  0.0868370
\end{verbatim}
\end{kframe}
\end{knitrout}
	}
	 Se vuelve a calcular el error estándar de los residuos. Atendiendo a la fórmula hay que dividir entre 7, ya que se tienen 7 entradas en el conjunto de datos del fichero. 
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(sr} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(res}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlnum{7}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 2.850242
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Una vez se tienen todos los datos necesarios, se pone a prueba el código visto previamente para detectar anomalías. Esta vez se cambiará el grado de outlier $d = 2$.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(res))\{}
        \hlkwa{if}\hlstd{(res[i]}\hlopt{>}\hlnum{2}\hlopt{*}\hlstd{sr)\{}
                \hlkwd{print}\hlstd{(}\hlstr{"el suceso"}\hlstd{);}
                \hlkwd{print}\hlstd{(res[i]);}
                \hlkwd{print}\hlstd{(}\hlstr{"es un suceso anómalo o outlier"}\hlstd{);}
                 \hlstd{\}}
\hlstd{\}}
\end{alltt}
\begin{verbatim}
## [1] "el suceso"
##      2. 
## 6.18587 
## [1] "es un suceso anómalo o outlier"
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Se observa en la salida del código que el suceso 2 es un outlier. Atendiendo a los parámetros que se han ido obteniendo, se observa a simple vista que el suceso 2 supera el límite establecido. 
	 
	 \chapter{Ejercicios autónomos}\pagestyle{fancy}
	 
	 En esta segunda parte de la práctica se implementarán los algoritmos vistos anteriormente, plasmando en el documento la explicación del código realizado, así como su aplicación a los enunciados propuestos para esta parte de la práctica.
	 
	 \section{Clasificación No Supervisada con K-Means}
	 
	 \begin{exercise}
	 	El primer conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con Kmeans, estará formado por los siguientes 15 valores de velocidades de respuesta y temperaturas normalizadas de un microprocesador \{Velocidad, Temperatura\}: 1.\{3.5, 4.5\}; 2.\{0.75, 3.25\}; 3.\{0, 3\}; 4.\{1.75, 0.75\}; 5.\{3, 3.75\}; 6.\{3.75, 4.5\}; 7.\{1.25, 0.75\}; 8.\{0.25, 3\}; 9.\{3.5, 4.25\}; 10.\{1.5, 0.5\}; 11.\{1, 1\}; 12.\{3, 4\}; 13.\{0.5, 3\}; 14.\{2, 0.25\}; 15.\{0, 2.5\}. Del análisis visual de los datos se ha concluido que hay una alta probabilidad que sean tres clusters.
	 \end{exercise}
	 
	 Los datos que se proponen en el enunciado del ejercicio van a ser introducidos en un fichero Excel (\texttt{.xlsx}), de tal forma que estos se van a obtener leyendo este fichero desde R. Para ello, se hará uso de la función \texttt{read.xlsx} contenida en el paquete \texttt{openxlsx} que se ha visto previamente. Con esta función se puede pasar como parámetro la ruta del archivo que contiene los datos, devolviendo los mismos en un dataframe.\\
	 
		 Para este ejercicio en concreto se hará uso de dos ficheros, uno con los datos de velocidad y temperatura (dispuestos cada uno en una columna del Excel), y otro fichero que contenga las coordenadas de los centroides iniciales que se van a utilizar en el algoritmo. En la primera fila de ambos ficheros deberá ir el título de cada columna. \\
	 
	 Primeramente se leen los datos del ejercicio. A partir de ahora, a cada uno de estos datos se les conocerán como puntos.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(data} \hlkwb{=} \hlkwd{data.frame}\hlstd{(}\hlkwd{read.xlsx}\hlstd{(}\hlstr{"data/datosKMeans.xlsx"}\hlstd{)))}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in read.xlsx("{}data/datosKMeans.xlsx"{}): no se pudo encontrar la función "{}read.xlsx"{}}}\end{kframe}
\end{knitrout}
	 
	 Y posteriormente se lee el fichero con los centroides iniciales. En la clasificación final se tendrán tantos clusters como centroides se hayan introducido, ya que al final un centroide representa un cluster.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(centroides} \hlkwb{=} \hlkwd{data.frame}\hlstd{(}\hlkwd{read.xlsx}\hlstd{(}\hlstr{"data/centroides.xlsx"}\hlstd{)))}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in read.xlsx("{}data/centroides.xlsx"{}): no se pudo encontrar la función "{}read.xlsx"{}}}\end{kframe}
\end{knitrout}
	 
	 Una vez se tienen todos los datos en el entorno de trabajo se va a proceder a explicar la implementación del algoritmo. Previo a ello se necesitan dos funciones auxiliares que ya se han visto en la anterior práctica. Estas son la función \texttt{len} para determinar la longitud de una lista y \texttt{euc\_distance}, a la cual se pasan dos puntos y devuelve la distancia euclídea entre los mismos.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{len} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{list}\hlstd{) \{}
                 \hlstd{count} \hlkwb{=} \hlnum{0}
                 \hlkwa{for} \hlstd{(element} \hlkwa{in} \hlstd{list) \{}
                        \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{+} \hlnum{1}
                 \hlstd{\}}
                 \hlstd{count}
\hlstd{\}}

\hlstd{euc_distance} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{p1}\hlstd{,} \hlkwc{p2}\hlstd{) \{}
        \hlkwd{sqrt}\hlstd{(((p1[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{p2[}\hlnum{1}\hlstd{])}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlstd{((p1[}\hlnum{2}\hlstd{]} \hlopt{-} \hlstd{p2[}\hlnum{2}\hlstd{])}\hlopt{^}\hlnum{2}\hlstd{))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	 
	 El primer paso del algoritmo consiste en realizar una matriz de distancias. Esta matriz tendrá tantas filas como centroides se hayan declarado y tantas columnas como puntos se hayan introducido. Esta matriz recoge las distancias euclídeas de todos los puntos a todos los centroides. Para realizar esta tabla se ha programado la siguiente función, la cual va rellenando una matriz con las distancias euclídeas de cada punto a cada cluster.
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{create_distance_matrix} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{points}\hlstd{,} \hlkwc{centroids}\hlstd{) \{}
                \hlstd{n_pts} \hlkwb{=} \hlkwd{nrow}\hlstd{(points)}
                \hlstd{n_cent} \hlkwb{=} \hlkwd{nrow}\hlstd{(centroids)}
                \hlstd{distances} \hlkwb{=} \hlkwd{data.frame}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlkwc{ncol} \hlstd{= n_pts,} \hlkwc{nrow} \hlstd{= n_cent))}
                \hlkwa{for} \hlstd{(cent} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_cent) \{}
                        \hlkwa{for} \hlstd{(pt} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_pts) \{}
                                \hlstd{distances[cent, pt]} \hlkwb{=}
                                \hlkwd{euc_distance}\hlstd{(points[pt,], centroids[cent,])}
                        \hlstd{\}}
        \hlstd{\}}
                \hlstd{distances}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	 }
	 
	 Una vez se tiene la matriz de distancias se debe realizar una matriz de asignaciones. Esta matriz será de las mismas dimensiones que la anterior, pero ahora no contendrá las distancias euclídeas. Por cada punto (columna) tendremos un 0 o un 1. Si aparece un 1 en una posición, la distancia del punto a ese cluster que representa la fila es la mínima. Por ejemplo; si el punto 1 esta más cerca del cluster 2 que del resto de clusters, la posición \texttt{$1x2$} tendrá un 1, y el resto de la columna un 0.\\
	 
	 Para realizar esta tabla se itera punto por punto, y por cada uno se observa la distancia del mismo a todos los clusters. Se debe poner un 1 en el lugar donde se encuentra la distancia euclídea mínima y un 0 en el resto de celdas de la columna. Es por ello que primero se inicializa a 0 toda la matriz y se va buscando por cada punto el cluster que está a menos distancia, para poner un 1 en su celda correspondiente. El código que recoge esta funcionalidad es el siguiente:
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{create_assigment_matrix} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{dist_matrix}\hlstd{) \{}
                 \hlstd{n_pts} \hlkwb{=} \hlkwd{ncol}\hlstd{(dist_matrix)}
                 \hlstd{n_cent} \hlkwb{=} \hlkwd{nrow}\hlstd{(dist_matrix)}
                 \hlstd{assig_matrix} \hlkwb{=} \hlkwd{data.frame}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{ncol} \hlstd{= n_pts,} \hlkwc{nrow} \hlstd{= n_cent))}
                 \hlkwa{for} \hlstd{(col} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_pts) \{}
                         \hlstd{minimum_centroid} \hlkwb{=} \hlnum{1}
                         \hlkwa{for} \hlstd{(cent} \hlkwa{in} \hlnum{2}\hlopt{:}\hlstd{n_cent) \{}
                                 \hlkwa{if} \hlstd{(dist_matrix[cent, col]} \hlopt{<}
                                   \hlstd{dist_matrix[minimum_centroid, col]) \{}
                                        \hlstd{minimum_centroid} \hlkwb{=} \hlstd{cent}
                                \hlstd{\}}
                         \hlstd{\}}
                         \hlstd{assig_matrix[minimum_centroid, col]} \hlkwb{=} \hlnum{1}
                 \hlstd{\}}
                 \hlstd{assig_matrix}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	}
	 
	 Por último, una vez se han asignado los puntos más próximos a cada cluster, se deben recalcular los centroides del mismo. Para recalcular un centroide en específico se deben tener en cuenta todos los puntos asociados al mismo. Las nuevas coordenadas del centroide se calculan realizando la media de los puntos que tiene asignados. El resultado de este algoritmo resulta en un nuevo dataframe donde, por cada fila, se van a tener las nuevas componentes del centroide correspondiente, y que serán usados para continuar con la ejecución del algoritmo.\\
	 
	 Antes de ver el código, se ha de considerar que un centroide puede no tener ningún punto asociado, por lo que el cálculo que se propone arriba no sería posible (ya que se tendría que dividir entre cero a la hora de calcular la media). Es por ello que si un centroide no tiene ningún punto asociado, este se quedará igual (sus componentes no se verán alteradas). Esto resultará en que el centroide propuesto no se ha definido muy bien, y desemboca en una clasificación diferente a la esperada.
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{update_cent} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{df_sample}\hlstd{,} \hlkwc{centroids}\hlstd{,} \hlkwc{assig_matrix}\hlstd{) \{}
                 \hlstd{n_pts} \hlkwb{=} \hlkwd{ncol}\hlstd{(assig_matrix)}
                 \hlstd{n_cent} \hlkwb{=} \hlkwd{nrow}\hlstd{(assig_matrix)}
                 \hlstd{new_centroids} \hlkwb{=} \hlkwd{c}\hlstd{()}
                 \hlkwa{for} \hlstd{(cent} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_cent) \{}
                         \hlstd{centroid_x} \hlkwb{=} \hlnum{0}
                         \hlstd{centroid_y} \hlkwb{=} \hlnum{0}
                         \hlstd{count} \hlkwb{=} \hlnum{0}
                         \hlkwa{for} \hlstd{(pt} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_pts) \{}
                                 \hlkwa{if} \hlstd{(assig_matrix[cent, pt]} \hlopt{==} \hlnum{1}\hlstd{) \{}
                                         \hlstd{centroid_x} \hlkwb{=} \hlstd{centroid_x} \hlopt{+} \hlstd{df_sample[pt,} \hlnum{1}\hlstd{]}
                                         \hlstd{centroid_y} \hlkwb{=} \hlstd{centroid_y} \hlopt{+} \hlstd{df_sample[pt,} \hlnum{2}\hlstd{]}
                                         \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{+} \hlnum{1}
                                 \hlstd{\}}
                         \hlstd{\}}
                         \hlkwa{if} \hlstd{(count} \hlopt{==} \hlnum{0}\hlstd{)\{}
                                \hlstd{new_centroids} \hlkwb{=} \hlkwd{append}\hlstd{(}\hlkwd{append}\hlstd{(new_centroids,}
                                   \hlstd{centroids[cent,}\hlnum{1}\hlstd{]),centroids[cent,}\hlnum{2}\hlstd{])}
                         \hlstd{\}}
                         \hlkwa{else} \hlstd{\{}
                                \hlstd{new_centroids} \hlkwb{=} \hlkwd{append}\hlstd{(}\hlkwd{append}\hlstd{(new_centroids,}
                                   \hlstd{centroid_x}\hlopt{/}\hlstd{count), centroid_y}\hlopt{/}\hlstd{count)}
                         \hlstd{\}}
                 \hlstd{\}}
                 \hlkwd{data.frame}\hlstd{(}\hlkwd{t}\hlstd{(}\hlkwd{matrix}\hlstd{(new_centroids,} \hlnum{2}\hlstd{, n_cent,}
                    \hlkwc{dimnames}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"X"}\hlstd{,}\hlstr{"Y"}\hlstd{)))))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	}
	 
	 El algoritmo se ejecutará hasta que la matriz de asignaciones sea la misma en una iteración con respecto a su anterior. Para visualizar el resultado o clasificación final se propone una función que tomará la matriz de asignaciones final y el valor de los centroides finales. Este método imprimirá las coordenadas de cada centroide, así como los puntos que conforman el cluster representado por ese centroide.
	 
	 {\tiny
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{printCentroids} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{df_sample}\hlstd{,} \hlkwc{centroids}\hlstd{,} \hlkwc{assig_matrix}\hlstd{)\{}
                 \hlstd{n_cents} \hlkwb{=} \hlkwd{nrow}\hlstd{(assig_matrix)}
                 \hlstd{n_pts} \hlkwb{=} \hlkwd{ncol}\hlstd{(assig_matrix)}
                 \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}n--RESULTADOS--\textbackslash{}n"}\hlstd{)}
                 \hlkwa{for} \hlstd{(cent} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_cents)\{}
                         \hlkwd{cat}\hlstd{(}\hlstr{"Centroide"}\hlstd{, cent,} \hlstr{"("}\hlstd{, centroids[cent,}\hlnum{1}\hlstd{],} \hlstr{","}\hlstd{, centroids[cent,}\hlnum{2}\hlstd{],}\hlstr{") \textbackslash{}n"}\hlstd{)}
                         \hlkwa{for}\hlstd{(pt} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_pts)\{}
                                 \hlkwa{if} \hlstd{(assig_matrix[cent,pt]} \hlopt{==} \hlnum{1}\hlstd{)\{}
                                        \hlkwd{cat}\hlstd{(}\hlstr{"  - Punto"}\hlstd{, pt,} \hlstr{"("}\hlstd{, df_sample[pt,} \hlnum{1}\hlstd{],} \hlstr{","}\hlstd{, df_sample[pt,}\hlnum{2}\hlstd{],} \hlstr{")\textbackslash{}n"}\hlstd{)}
                                 \hlstd{\}}
                         \hlstd{\}}
                         \hlkwd{cat}\hlstd{(}\hlstr{"\textbackslash{}n"}\hlstd{)}
                 \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	}
	 
	 De todo este algoritmo, se ha considerado que la salida mas útil para el analista es un vector de clusterización. Este consiste en un vector de tantos elementos como puntos se hayan introducido, de tal forma que la posición $i$-ésima del mismo corresponda con el cluster asociado al punto $i$. Se itera la matriz de asignaciones final por puntos, y por cada uno de ellos se devuelve el valor de la fila que contiene un 1 (el número del centroide al que se ha asociado en la clasificación final)
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{create_clust_vector} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{assig_matrix}\hlstd{)\{}
                 \hlstd{n_pts} \hlkwb{=} \hlkwd{ncol}\hlstd{(assig_matrix)}
                 \hlstd{n_cents} \hlkwb{=} \hlkwd{nrow}\hlstd{(assig_matrix)}
                 \hlstd{clust_vector} \hlkwb{=} \hlkwd{c}\hlstd{()}
                 \hlkwa{for}\hlstd{(pt} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_pts)\{}
                         \hlkwa{for}\hlstd{(cent} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n_cents)\{}
                                 \hlkwa{if} \hlstd{(assig_matrix[cent,pt]} \hlopt{==} \hlnum{1}\hlstd{)\{}
                                        \hlstd{clust_vector} \hlkwb{=} \hlkwd{append}\hlstd{(clust_vector, cent)}
                                 \hlstd{\}}
                         \hlstd{\}}
                 \hlstd{\}}
                 \hlstd{clust_vector}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	}
	 
	 Todas estas funciones se recogen en la función \texttt{fcd\_kmeans}. Esta función será la que implemente el algoritmo y a la que se deberá llamar. Recibirá como parámetros un dataframe con los datos de la muestra, otro dataframe con los centroides iniciales y un booleano \texttt{details} el cual se usará para recibir una salida más o menos detallada. En caso de que se quiera una salida detallada, se imprimirán paso por paso la matriz de distancias, la matriz de asignaciones y los valores de los nuevos centroides en ese paso concreto. En caso contrario simplemente se imprimirá la clasificación final (se llama a la función \texttt{printCentroids}) y se devolverá el vector de clusterización. Por defecto, \texttt{details} se encuentra a \texttt{FALSE}, ofreciendo una salida no detallada en caso de no indicar este parámetro en la llamada. El usuario podrá asignar a una variable la llamada a la función, recibiendo en la misma el vector de clusterización.
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fcd_kmeans} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{df_sample}\hlstd{,} \hlkwc{centroids}\hlstd{,} \hlkwc{details} \hlstd{=} \hlnum{FALSE}\hlstd{) \{}
                 \hlstd{n_pts} \hlkwb{=} \hlkwd{nrow}\hlstd{(df_sample)}
                 \hlstd{n_cent} \hlkwb{=} \hlkwd{nrow}\hlstd{(centroids)}

                 \hlstd{prev_assig_matrix} \hlkwb{=} \hlkwd{data.frame}\hlstd{(}
                     \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{ncol} \hlstd{= n_pts,} \hlkwc{nrow} \hlstd{= n_cent))}

                 \hlstd{step} \hlkwb{=} \hlnum{1}
                 \hlstd{dist_matrix} \hlkwb{=} \hlkwd{create_distance_matrix}\hlstd{(df_sample, centroids)}
                 \hlstd{assig_matrix} \hlkwb{=} \hlkwd{create_assigment_matrix}\hlstd{(dist_matrix)}
                 \hlkwa{if}\hlstd{(details)\{}
                         \hlkwd{cat}\hlstd{(}\hlstr{"PASO"}\hlstd{, step,} \hlstr{"\textbackslash{}n------- \textbackslash{}n"}\hlstd{)}
                         \hlkwd{cat}\hlstd{(}\hlstr{"Matriz de distancias \textbackslash{}n"}\hlstd{)}
                         \hlkwd{print}\hlstd{(dist_matrix)}
                         \hlkwd{cat}\hlstd{(}\hlstr{"Matriz de asignaciones \textbackslash{}n"}\hlstd{)}
                         \hlkwd{print}\hlstd{(assig_matrix)}
                 \hlstd{\}}

                 \hlkwa{while} \hlstd{(}\hlopt{!}\hlkwd{identical}\hlstd{(prev_assig_matrix, assig_matrix)) \{}
                         \hlstd{prev_assig_matrix} \hlkwb{=} \hlstd{assig_matrix}
                         \hlstd{centroids} \hlkwb{=} \hlkwd{update_cent}\hlstd{(df_sample, centroids, assig_matrix)}
                         \hlstd{dist_matrix} \hlkwb{=} \hlkwd{create_distance_matrix}\hlstd{(df_sample, centroids)}
                         \hlstd{assig_matrix} \hlkwb{=} \hlkwd{create_assigment_matrix}\hlstd{(dist_matrix)}

                         \hlkwa{if}\hlstd{(details) \{}
                                 \hlkwd{cat}\hlstd{(}\hlstr{"Los nuevos centroides son:\textbackslash{}n"}\hlstd{)}
                                 \hlkwd{print}\hlstd{(centroids)}
                                 \hlstd{step} \hlkwb{=} \hlstd{step} \hlopt{+} \hlnum{1}
                                 \hlkwd{cat}\hlstd{(}\hlstr{"PASO"}\hlstd{, step,} \hlstr{"\textbackslash{}n------- \textbackslash{}n"}\hlstd{)}
                                 \hlkwd{cat}\hlstd{(}\hlstr{"Matriz de distancias \textbackslash{}n"}\hlstd{)}
                                 \hlkwd{print}\hlstd{(dist_matrix)}
                                 \hlkwd{cat}\hlstd{(}\hlstr{"Matriz de asignaciones \textbackslash{}n"}\hlstd{)}
                                 \hlkwd{print}\hlstd{(assig_matrix)}
                         \hlstd{\}}
                 \hlstd{\}}
                 \hlkwd{printCentroids}\hlstd{(df_sample, centroids, assig_matrix)}
                 \hlkwd{cat}\hlstd{(}\hlstr{"Vector de clusterización\textbackslash{}n"}\hlstd{)}
                 \hlkwd{create_clust_vector}\hlstd{(assig_matrix)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	}
	 
	 Un aspecto a destacar es el uso de la función \texttt{identical} contenida en el paquete \texttt{base}. Esta función permite comparar si dos dataframes son iguales. Esto sirve en el algoritmo para ver si la matriz de asignaciones de la anterior iteración es igual que la actual. Esta condición será la que se deba cumplir para que el algoritmo finalice.\\
	 
	 Con esto ya estaría el algoritmo implementado. Ahora se va a llamar a la función \texttt{fcd\_kmeans} con los datos leídos al principio. Se va a optar por una salida detallada.
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{fcd_kmeans}\hlstd{(data, centroides,} \hlnum{TRUE}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(expr, envir, enclos): objeto 'centroides' no encontrado}}\end{kframe}
\end{knitrout}
	}
	 
	 Se ha optado por tres centroides que más o menos se encuentran entre el conjunto total de puntos. Han sido necesarios 4 pasos para llegar a una solución. Se puede ver cómo se han ido calculando las matrices de distancias y asignaciones, así como los nuevos centroides, hasta que se ha llegado a los resultados finales. En estos se aprecian las coordenadas de los tres centroides así como los puntos que contienen cada uno. Por último, se devuelve el vector de clusterización que también se puede ver en la salida de la función realizada.
\end{document}
