\documentclass[12pt]{report}\usepackage[]{graphicx}\usepackage[dvipsnames]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[spanish]{babel}
\usepackage[margin = 2.54cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{amssymb, amsthm, array, cancel, enumitem, fancyhdr, float, graphicx, hyperref, hologo, listings, mathtools, qtree, tikz, tikz-cd}
\usepackage[spanish, noabbrev]{cleveref}

\pagestyle{fancy}
\lhead{\footnotesize \leftmark}
\rhead{\footnotesize \rightmark}

\lstdefinestyle{estilo_pablo}{
	basicstyle = \ttfamily\footnotesize, 
	tabsize = 2, 
	commentstyle = \color{gray}, 
	keywordstyle = \color{cyan}, 
	stringstyle = \color{purple}, 
	tabsize = 1, 
	frame = tb, 
	breaklines = true, 
	showstringspaces = false, 
	numbers = left, 
	numberstyle = \footnotesize\color{gray}, 
	stepnumber = 1, 
	captionpos = b
}

\crefname{listing}{Código}{Códigos}
\crefname{section}{Sección}{Secciones}

\title{
	\huge
	\noindent\textbf{Fundamentos de la Ciencia de Datos}\\
	
	{\Large \textit{Práctica 2}}
	\vspace{1cm}
	
	\huge
	Grado en Ingeniería Informática\\
	Universidad de Alcalá\\
	
	\vspace{1cm}
	
	\includegraphics[scale = 0.075]{img/logo}
}

\author{
	Grupo 9\\\\
	Pablo García García\\
	Abel López Martínez\\
	Álvaro Jesús Martínez Parra\\
	Raúl Moratilla Núñez
}

\date{
	\large{\today}
}

\hypersetup{
	pdftitle = {Práctica 2}, 
	pdfauthor = {Pablo García García, Abel López Martínez, Álvaro Jesús Martínez Parra, Raúl Moratilla Núñez}, 
	pdfsubject = {Fundamentos de la Ciencia de Datos}, 
	pdfcenterwindow, 
	pdfnewwindow = true, 
	pdfkeywords = {Entrega de la PL2 de laboratorio correspondiente al Curso 2023-2024}, 
	bookmarksopen = true 
}

\newtheorem{exercise}{Ejercicio}[section]
\newtheorem{observation}{Observación}[section]
\newtcbtheorem[number within = section, Crefname = {Teorema}{Teorema}]{teorema}{Teorema}{fonttitle = \bfseries}{th}
\newtcbtheorem[number within = section, Crefname = {Definición}{Definición}]{definicion}{Definición}{fonttitle = \bfseries}{df}

\newcommand{\dt}{\text{dist}}
\newcommand{\ds}{\text{dens}}
\newcommand{\drm}{\text{drm}}

\newcommand{\rojo}[1]{\textcolor{Red}{#1}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
	
	\renewcommand{\chaptername}{Parte}
	\renewcommand{\lstlistingname}{Código}
	\maketitle \thispagestyle{empty}
	
	\newpage
	
	\setcounter{tocdepth}{3}
	\tableofcontents
	
	\chapter*{Introducción}\addcontentsline{toc}{chapter}{Introducción}\pagestyle{plain}
	
	\chapter{Ejercicios guiados}\pagestyle{fancy}
	
	En esta primera parte de la práctica, se repetirán los ejercicios explicados y realizados por el profesor en las clases de laboratorio, utilizando los mismos procedimientos vistos y plasmándolos en este documento.
	
	\section{Clasificación No Supervisada con K-Means}
	
	\begin{exercise}
		El primer conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con KMeans, estará formado por las siguientes 8 calificaciones de estudiantes: 1. \{4, 4\}; 2. \{3, 5\}; 3. \{1, 2\}; 4. \{5, 5\}; 5.\{0, 1\}; 6. \{2, 2\}; 7. \{4, 5\}; 8. \{2, 1\}, donde las características de las calificaciones son: \{Teoría, Laboratorio\}.
	\end{exercise}
	
	En este ejercicio se va a detallar cómo realizar una clasificación no supervisada en un conjunto de datos en R; concretamente utilizando el algoritmo K-Means. El objetivo será agrupar subconjuntos de los datos en clusters o grupos, creando de esta forma una clasificación del conjunto total. Estos clusters se determinarán en base a la muestra, obteniéndose durante el mismo proceso de clasificación.\\
	
	El algoritmo K-Means ofrece una técnica de clusterización en base a una muestra de datos y una cantidad $n$ de clusters. Para empezar a realizar el algoritmo se necesita la ubicación de los centroides de cada cluster. Un centroide es el punto medio del grupo de sucesos que componen el cluster, por lo que habrá tantos centroides como $n$ clusters haya. La cantidad de clusters así como sus centroides iniciales serán elegidos arbitrariamente por el usuario; estos se irán reubicando a medida que se itere en el algoritmo. \\
	
	Para empezar, en R se necesita introducir la muestra o conjunto de datos con el que se va a trabajar. Estos datos serán introducidos en una matriz utilizando la función \texttt{matrix}.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m}\hlkwb{<-}\hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlnum{4}\hlstd{,} \hlnum{3}\hlstd{,}\hlnum{5}\hlstd{,} \hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,} \hlnum{5}\hlstd{,}\hlnum{5}\hlstd{,} \hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,}\hlnum{2}\hlstd{,} \hlnum{4}\hlstd{,}\hlnum{5}\hlstd{,} \hlnum{2}\hlstd{,}\hlnum{1}\hlstd{),}\hlnum{2}\hlstd{,}\hlnum{8}\hlstd{)}
\hlstd{(m}\hlkwb{<-}\hlkwd{t}\hlstd{(m))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    4    4
## [2,]    3    5
## [3,]    1    2
## [4,]    5    5
## [5,]    0    1
## [6,]    2    2
## [7,]    4    5
## [8,]    2    1
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Para poder trabajar debidamente se debe trasponer la matriz, de tal forma que cada columna represente una componente. El primer punto conformará la primera fila, el segundo punto la segunda fila y así sucesivamente con todos los datos.\\
	
	Además del conjunto de datos, el algoritmo K-Means necesita unos centroides iniciales. Estos se introducirán de la misma forma que la muestra, teniendo en la primera fila el centroide del primer cluster, en la siguiente fila el centroide del segundo... Al tratarse de pocos puntos se eligen arbitrariamente dos centroides, por lo que la clasificación final será realizada con dos clusters. Se introducen así los centroides:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{c}\hlkwb{<-}\hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{),}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{)}
\hlstd{(c}\hlkwb{<-}\hlkwd{t}\hlstd{(c))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    0    1
## [2,]    2    2
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Con la muestra y los centroides introducidos en el entorno de trabajo, ya se puede realizar el algoritmo. Para ello se utilizará la función \texttt{kmeans}, función del paquete cargado por defecto \texttt{stats}. La función recibe tres parámetros: la muestra de los datos (\texttt{m}), los centroides iniciales de los clusters (\texttt{c}) y el número de iteraciones que se desean. En este caso se eligen cuatro iteraciones, las mismas que se necesitaron en clase de teoría. En teoría, el número de iteraciones no se sabe a priori, por lo que habría que ir probando. Sin embargo, como ya se sabe el número que se necesita, se pone directamente 4.
	
	{\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(clasificacionns} \hlkwb{=} \hlstd{(}\hlkwd{kmeans}\hlstd{(m,c,}\hlnum{4}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## K-means clustering with 2 clusters of sizes 4, 4
## 
## Cluster means:
##   [,1] [,2]
## 1 1.25 1.50
## 2 4.00 4.75
## 
## Clustering vector:
## [1] 2 2 1 2 1 1 2 1
## 
## Within cluster sum of squares by cluster:
## [1] 3.75 2.75
##  (between_SS / total_SS =  84.8 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"
\end{verbatim}
\end{kframe}
\end{knitrout}
	}
	
	Entre los parámetros que aparecen en la salida se encuentran \texttt{Cluster means} y \texttt{Clustering vector}. El primero de ellos proporciona la ubicación de los dos centroides de los clusters que conforman la clasificación final. Como se puede apreciar, salen los dos centroides que salieron en teoría. El segundo parámetro indica a qué cluster pertenece cada suceso de la muestra introducida. Así el punto 1 pertenece al cluster 2, el punto 2 pertenece al cluster 2, el punto 3 al cluster 1... Este último se puede utilizar como entrada para realizar otras tareas. Puede ser muy útil para analizar cada cluster por separado, ver sus caracteristicas comunes e intentar deducir por qué esos datos están relacionados.\\
	
	El siguiente comando permite añadir una columna a la izquierda de la muestra de datos (\texttt{m}) que se había introducido al inicio, indicando a qué cluster pertenece cada suceso.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(m}\hlkwb{=}\hlkwd{cbind}\hlstd{(clasificacionns}\hlopt{$}\hlstd{cluster,m))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    2    4    4
## [2,]    2    3    5
## [3,]    1    1    2
## [4,]    2    5    5
## [5,]    1    0    1
## [6,]    1    2    2
## [7,]    2    4    5
## [8,]    1    2    1
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Esto se realiza para poder dividir la muestra según la clasificación que se ha conseguido. Llamando a la función subset se puede extraer la porción de la muestra que pertenece al cluster 1 y la porción restante que pertenece al cluster 2.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(mc1}\hlkwb{=}\hlkwd{subset}\hlstd{(m,m[,}\hlnum{1}\hlstd{]}\hlopt{==}\hlnum{1}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    1    2
## [2,]    1    0    1
## [3,]    1    2    2
## [4,]    1    2    1
\end{verbatim}
\begin{alltt}
\hlstd{(mc2}\hlkwb{=}\hlkwd{subset}\hlstd{(m,m[,}\hlnum{1}\hlstd{]}\hlopt{==}\hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    2    4    4
## [2,]    2    3    5
## [3,]    2    5    5
## [4,]    2    4    5
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Una vez se tienen los datos separados se puede eliminar la columna que indica el cluster al que pertenece cada dato, ya que se sobreentiende que todos pertenecen al mismo cluster porque ya han pasado por un proceso de separación en función de la clasificación. Por ejemplo, para los datos pertenecientes al primer cluster, se haría de la siguiente forma:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(mc1} \hlkwb{=} \hlstd{mc1[,}\hlopt{-}\hlnum{1}\hlstd{])}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    1    2
## [2,]    0    1
## [3,]    2    2
## [4,]    2    1
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Con todo esto se consigue, partiendo de una muestra de datos, una clasificación no supervisada utilizando la técnica de clusterización basada en el algoritmo K-Means. A partir de ella se podrán intentar deducir ciertas conclusiones.
	
	\section{Clasificación No Supervisada con CJA}
	
	\begin{exercise}
		El segundo conjunto de datos, que se empleará para realizar el análisis de clasificación no supervisada con Clusterización Jerárquica Aglomerativa, estará formado por 6 calificaciones de estudiantes: 1. \{0.89, 2.94\}; 2.\{4.36, 5.21\}; 3. \{3.75, 1.12\}; 4. \{6.25, 3.14\}; 5. \{4.1, 1.8\}; 6. \{3.9, 4.27\}.
	\end{exercise}
	
	La Clusterización Jerárquica Aglomerativa es un método de análisis de datos que comienza tratando cada punto como un cluster individual y luego, iterativamente, combina los clusters más cercanos hasta formar un único cluster. Este método es útil para identificar grupos naturales en los datos. En clase se implementó mediante el paquete \texttt{LearnClust} del CRAN.\\
	
	Inicialmente, se crea una matriz con la muestra del problema, y se transpone la matriz para que tenga el formato deseado.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}
        \hlkwd{c}\hlstd{(}\hlnum{0.89}\hlstd{,}\hlnum{2.94}\hlstd{,} \hlnum{4.36}\hlstd{,}\hlnum{5.21}\hlstd{,} \hlnum{3.75}\hlstd{,}\hlnum{1.12}\hlstd{,} \hlnum{6.25}\hlstd{,}\hlnum{3.14}\hlstd{,} \hlnum{4.1}\hlstd{,}\hlnum{1.8}\hlstd{,} \hlnum{3.9}\hlstd{,}\hlnum{4.27}\hlstd{),}\hlnum{2}\hlstd{,}\hlnum{6}\hlstd{)}
\hlstd{(m} \hlkwb{<-} \hlkwd{t}\hlstd{(m))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,] 0.89 2.94
## [2,] 4.36 5.21
## [3,] 3.75 1.12
## [4,] 6.25 3.14
## [5,] 4.10 1.80
## [6,] 3.90 4.27
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Para la ejecución del algoritmo utilizamos la función \texttt{agglomerativeHC}. Esta función recibe como parámetros la matriz de datos, la métrica de distancia y el criterio de enlace.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{agglomerativeHC}\hlstd{(m,} \hlstr{'EUC'}\hlstd{,} \hlstr{'MIN'}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-8-1} 
\begin{kframe}\begin{verbatim}
## $dendrogram
## Number of objects: 6 
## 
## 
## $clusters
## $clusters[[1]]
##     X1   X2
## 1 0.89 2.94
## 
## $clusters[[2]]
##     X1   X2
## 1 4.36 5.21
## 
## $clusters[[3]]
##     X1   X2
## 1 3.75 1.12
## 
## $clusters[[4]]
##     X1   X2
## 1 6.25 3.14
## 
## $clusters[[5]]
##    X1  X2
## 1 4.1 1.8
## 
## $clusters[[6]]
##    X1   X2
## 1 3.9 4.27
## 
## $clusters[[7]]
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
## 
## $clusters[[8]]
##     X1   X2
## 1 4.36 5.21
## 2 3.90 4.27
## 
## $clusters[[9]]
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
## 3 4.36 5.21
## 4 3.90 4.27
## 
## $clusters[[10]]
##     X1   X2
## 1 6.25 3.14
## 2 3.75 1.12
## 3 4.10 1.80
## 4 4.36 5.21
## 5 3.90 4.27
## 
## $clusters[[11]]
##     X1   X2
## 1 0.89 2.94
## 2 6.25 3.14
## 3 3.75 1.12
## 4 4.10 1.80
## 5 4.36 5.21
## 6 3.90 4.27
## 
## 
## $groupedClusters
##   cluster1 cluster2
## 1        3        5
## 2        2        6
## 3        7        8
## 4        4        9
## 5        1       10
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	En este código, `EUC' representa la métrica de distancia euclidiana y `MIN' el criterio para la agrupación de clusters. En la salida se pueden observar cuatro apartados.\\
	
	El primero es una figura los datos de la matriz que hemos introducido impresos en una gráfica.\\
	
	El segundo es el número de puntos que se han introducido, mediante el atributo \texttt{\$dendrogram}, en este caso 6.\\
	
	El tercer apartado es una lista que muestra las coordenadas de todos los clusters al finalizar la ejecución, los 6 primeros son los puntos introducidos y los 5 siguientes son los formados por cada una de las iteraciones uniendo dos clusters anteriores hasta tener todos unidos en el mismo cluster, momento en el que finaliza el algoritmo. Se puede ver en el atributo \texttt{\$clusters}.\\
	
	El cuarto apartado es el proceso que se realiza en cada iteración, se pueden ver en el apartado \texttt{\$groupedClusters}.
	
	\begin{enumerate}
		\item Se unen los clusters 3 y 5, para formar el cluster 7.
		\item Se unen los clusters 2 y 6, para formar el cluster 8.
		\item Se unen los clusters 4 y 8, para formar el cluster 9.
		\item Se unen los clusters 7 y 9, para formar el cluster 10.
		\item Se unen los clusters 1 y 10, para formar el cluster 11.
	\end{enumerate}
	
	Para obtener una explicación detallada paso a paso tal y como se ha explicado en clase, usamos la función \texttt{agglomerativeHC.details}.
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{agglomerativeHC.details}\hlstd{(m,} \hlstr{'EUC'}\hlstd{,} \hlstr{'MIN'}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ Agglomerative hierarchical clustering is a classification technique that initializes \\\#\# \ a cluster for each data.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ \ It calculates the distance between datas depending on the approach type given and}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ it creates a new cluster joining the most similar clusters until getting only one.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ 'toList' creates a list initializing datas by creating clusters with each one}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ \ These are the clusters with only one element:}}\begin{verbatim}
## [[1]]
##      [,1] [,2] [,3]
## [1,] 0.89 2.94    1
## 
## [[2]]
##      [,1] [,2] [,3]
## [1,] 4.36 5.21    1
## 
## [[3]]
##      [,1] [,2] [,3]
## [1,] 3.75 1.12    1
## 
## [[4]]
##      [,1] [,2] [,3]
## [1,] 6.25 3.14    1
## 
## [[5]]
##      [,1] [,2] [,3]
## [1,]  4.1  1.8    1
## 
## [[6]]
##      [,1] [,2] [,3]
## [1,]  3.9 4.27    1
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ \ In each step: \\\#\# \ \ \ - It calculates a matrix distance between active clusters depending on the approach and distance type.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ - It gets the minimum distance value from the matrix.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ - It creates a new cluster joining the minimum distance clusters.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \ \ \ - It repeats these steps while final clusters do not include all datas.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 1}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##          [,1]     [,2]      [,3]     [,4]      [,5]     [,6]
## [1,] 0.000000 4.146541 3.3899853 5.363730 3.4064204 3.290745
## [2,] 4.146541 0.000000 4.1352388 2.803034 3.4198977 1.046518
## [3,] 3.389985 4.135239 0.0000000 3.214094 0.7647876 3.153569
## [4,] 5.363730 2.803034 3.2140940 0.000000 2.5333969 2.607566
## [5,] 3.406420 3.419898 0.7647876 2.533397 0.0000000 2.478084
## [6,] 3.290745 1.046518 3.1535694 2.607566 2.4780839 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 0.764787552199956}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 3, 5}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 3 and cluster 5, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 2}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##          [,1]     [,2] [,3]     [,4] [,5]     [,6]     [,7]
## [1,] 0.000000 4.146541    0 5.363730    0 3.290745 3.389985
## [2,] 4.146541 0.000000    0 2.803034    0 1.046518 3.419898
## [3,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
## [4,] 5.363730 2.803034    0 0.000000    0 2.607566 2.533397
## [5,] 0.000000 0.000000    0 0.000000    0 0.000000 0.000000
## [6,] 3.290745 1.046518    0 2.607566    0 0.000000 2.478084
## [7,] 3.389985 3.419898    0 2.533397    0 2.478084 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 1.04651803615609}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 2, 6}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 2 and cluster 6, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 4.36 5.21
## 2 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 3}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##          [,1] [,2] [,3]     [,4] [,5] [,6]     [,7]     [,8]
## [1,] 0.000000    0    0 5.363730    0    0 3.389985 3.290745
## [2,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [3,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [4,] 5.363730    0    0 0.000000    0    0 2.533397 2.607566
## [5,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [6,] 0.000000    0    0 0.000000    0    0 0.000000 0.000000
## [7,] 3.389985    0    0 2.533397    0    0 0.000000 2.478084
## [8,] 3.290745    0    0 2.607566    0    0 2.478084 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 2.47808393723861}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 7, 8}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 7 and cluster 8, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 3.75 1.12
## 2 4.10 1.80
## 3 4.36 5.21
## 4 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 4}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##           [,1] [,2] [,3]     [,4] [,5] [,6] [,7] [,8]     [,9]
##  [1,] 0.000000    0    0 5.363730    0    0    0    0 3.290745
##  [2,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [3,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [4,] 5.363730    0    0 0.000000    0    0    0    0 2.533397
##  [5,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [6,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [7,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [8,] 0.000000    0    0 0.000000    0    0    0    0 0.000000
##  [9,] 3.290745    0    0 2.533397    0    0    0    0 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 2.53339692902632}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 4, 9}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 4 and cluster 9, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 6.25 3.14
## 2 3.75 1.12
## 3 4.10 1.80
## 4 4.36 5.21
## 5 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# STEP => 5}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Matrix Distance (distance type = EUC, approach type = MIN):}}\begin{verbatim}
##           [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]    [,10]
##  [1,] 0.000000    0    0    0    0    0    0    0    0 3.290745
##  [2,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [3,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [4,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [5,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [6,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [7,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [8,] 0.000000    0    0    0    0    0    0    0    0 0.000000
##  [9,] 0.000000    0    0    0    0    0    0    0    0 0.000000
## [10,] 3.290745    0    0    0    0    0    0    0    0 0.000000
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The minimum distance is: 3.29074459659208}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The closest clusters are: 1, 10}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The grouped clusters are added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ Grouping clusters 1 and cluster 10, it is created a new cluster:}}\begin{verbatim}
##     X1   X2
## 1 0.89 2.94
## 2 6.25 3.14
## 3 3.75 1.12
## 4 4.10 1.80
## 5 4.36 5.21
## 6 3.90 4.27
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ The new cluster is added to the solution.}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# \ This loop has been repeated until the last cluster contained every single clusters.}}\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-9-1} 
\end{knitrout}
	
	La función también se puede ejecutar con diferentes criterios de unión, como `MAX' y `AVG', para observar cómo cambian los clusters:\\
	
	\texttt{agglomerativeHC.details(m, `EUC', `MAX')}\\
	
	\texttt{agglomerativeHC.details(m, `EUC', `AVG')}
	
	\section{Clasificación Supervisada con Árboles de Decisión}
	
	\begin{exercise}
		El tercer conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando árboles de decisión, estará formado por las siguientes 9 calificaciones de estudiantes: 1. \{A, A, B, Ap\}; 2. \{A, B, D, Ss\}; 3. \{D, D, C, Ss\}; 4. \{D, D, A, Ss\}; 5. \{B, C, B, Ss\}; 6. \{C, B, B, Ap\}; 7. \{B, B, A, Ap\}; 8. \{C, D, C, Ss\}; 9. \{B, A, C,
		Ss\}, donde las características de las calificaciones son: \{Teoría, Laboratorio, Prácticas, Calificación Global).
	\end{exercise}
	
	\newpage
	
	\section{Clasificación Supervisada con Regresión}
	
	\begin{exercise}
		El cuarto conjunto de datos, que se empleará para realizar el análisis de clasificación supervisada utilizando regresión, estará formado por los siguientes 4 radios ecuatoriales y densidades de los planetas interiores:
		\{Mercurio, 2.4, 5.4; Venus, 6.1, 5.2; Tierra, 6.4, 5.5; Marte, 3.4, 3.9\}.
	\end{exercise}
	
	Para empezar este ejercicio se deberán introducir los datos expuestos en el enunciado en un archivo de texto (\texttt{.txt}), con el fin de ser posteriormente leídos. La inserción de los datos en este fichero se hará atendiendo a las normas relacionadas con este tipo de archivos que ya se vieron en la primera práctica. Estas normas son las siguientes:
	
	\begin{itemize}
		\item Existirá una tabulación entre dato y dato. 
		\item La primera columna numera las filas, y en la primera fila se introduce un espacio y el nombre de las variables. 
		\item Se introducirá un salto de línea en la última fila. 
		\item Para los números decimales se utilizarán puntos. 
		\item Al escribir nombres, no se deberán introducir espacios. 
	\end{itemize}
	
	Obedeciendo estas normas, se copian los datos en un fichero llamado \texttt{planetas.txt}, y se carga en R de la siguiente manera:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(muestra} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"data/planetas.txt"}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      R   D
## 1. 2.4 5.4
## 2. 6.1 5.2
## 3. 6.4 5.5
## 4. 3.4 3.9
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	Una vez se tienen los datos en R, se procede a hacer uso de la función \texttt{lm} contenida en los paquetes básicos (concretamente en el paquete \texttt{stats}). Esta función recibe dos parámetros. El primero de ellos es la fórmula en donde se va a decir en función de qué parámetro se quiere otro parámetro. En este caso se tiene la columna \texttt{R} que representa el radio y la columna \texttt{D} que representa la densidad. Como se quiere la densidad en función del radio, el primer parámetro tendrá que ser formula=D$\sim$R. Se indica de esta forma que se pretende obtener la columna \texttt{D} en función de la columna \texttt{R}; o lo que es lo mismo, la densidad en función del radio.\\
	
	El segundo parámetro que entra a la función es la estructura que contiene los datos que se pretenden estudiar. En este caso, el parámetro \texttt{data} será la variable \texttt{muestra}, confeccionada previamente.\\
	
	Con los parámetros de la función \texttt{lm} claros, se invoca a la misma de la siguiente forma:
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(regresion}\hlkwb{=}\hlkwd{lm}\hlstd{(D}\hlopt{~}\hlstd{R,} \hlkwc{data}\hlstd{=muestra))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = D ~ R, data = muestra)
## 
## Coefficients:
## (Intercept)            R  
##      4.3624       0.1394
\end{verbatim}
\end{kframe}
\end{knitrout}
	
	En la salida de la función se observan los coeficientes que conforman la recta de regresión que mejor se adapta a los datos introducidos. El método de obtención o ajuste de la función es el de mínimos cuadrados, pudiendo comprobar que el resultado es el mismo que el que se ha visto en clase. En este método, los coeficientes se calculan de la siguiente forma, siendo $x$ el radio e $y$ la densidad:
	
	\begin{center}
		$b = \frac{s_{xy}}{s^{2}_x}$;
		$a = \overline{y} -b\overline{x}$
	\end{center}
	 
	 La salida proporciona directamente los valores de a y b, siendo estos el primero y el segundo respectivamente. Con estos valores se puede decir que la densidad de un planeta se puede sacar atendiendo a la siguiente fórmula:
	 
	 \begin{center}
	 	$D = 4.3624 + 0.1394R$
	 \end{center}
	 
	 Con el comando \texttt{summary} aplicado a la regresión que se acaba de hacer se pueden ver parámetros que detallan esta recta de regresión con respecto a los datos.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(}\hlkwd{summary}\hlstd{(regresion))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = D ~ R, data = muestra)
## 
## Residuals:
##       1.       2.       3.       4. 
##  0.70312 -0.01253  0.24566 -0.93624 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)  
## (Intercept)   4.3624     1.2050   3.620   0.0685 .
## R             0.1394     0.2466   0.565   0.6289  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.846 on 2 degrees of freedom
## Multiple R-squared:  0.1377,	Adjusted R-squared:  -0.2935 
## F-statistic: 0.3193 on 1 and 2 DF,  p-value: 0.6289
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Entre todos los detalles que aparecen, se observa el parámetro \texttt{Residuals}. Este parámetro devuelve un vector con los residuos, o dicho de otra forma, la distancia entre el valor real y el valor que se obtiene por medio de la recta calculada. También se puede apreciar el parámetro \texttt{Multiple R-squared}, el cual sirve como medida de cuán bueno ha sido el ajuste. Este valor podrá tomar valores entre 0 y 1, siendo 0 un ajuste muy malo y 1 un ajuste perfecto. Como se observa, el valor de este parámetro es 0.1377, lo cual quiere decir que el ajuste es malo; y con ello se puede concluir que el radio no explica la densidad de los planetas.\\
	 
	 Una vez visto en detalle el cálculo y valoración de la recta de regresión se va a ver cómo se pueden identificar sucesos anómalos mediante la técnica de regresión. El proceso por lo general sigue 5 pasos:
	 
	 \begin{enumerate}
	 	\item Determinar el grado de outlier \texttt{d}
	 	\item Obtener la ecuación de la recta de regresión
	 	\item Obtener el error estándar $s_r$ del vector de residuos
	 	\item Calcular el límite para los valores típicos como $lim = d \cdot s_r$
	 	\item Identificar como outliers los residuos (en valor absoluto) que superen ese límite
	 \end{enumerate}
	 
	 Hasta ahora se tiene la recta de regresión y el vector de residuos que está en \texttt{summary}. Para extraerlo y poder operar con él se hace de la siguiente forma:
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(res}\hlkwb{=}\hlkwd{summary}\hlstd{(regresion)}\hlopt{$}\hlstd{residuals)}
\end{alltt}
\begin{verbatim}
##          1.          2.          3.          4. 
##  0.70312301 -0.01253452  0.24565541 -0.93624389
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Una vez se tiene el vector de residuos se calcula el error estándar del mismo:
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(sr} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(res}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlnum{4}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 0.5982136
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Con ello se puede plantear un bucle el cual compruebe qué elemento o elementos se presentan como anomalías. Se va a elegir como grado de outlier $d = 3$
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(res))\{}
                 \hlkwa{if}\hlstd{(res[i]}\hlopt{>}\hlnum{3}\hlopt{*}\hlstd{sr)\{}
                        \hlkwd{print}\hlstd{(}\hlstr{"el suceso"}\hlstd{);}
                        \hlkwd{print}\hlstd{(res[i]);}
                        \hlkwd{print}\hlstd{(}\hlstr{"es un suceso anómalo o outlier"}\hlstd{)}
                 \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	 
	 Este bucle comprueba por cada elemento del vector de residuos si supera o no el límite establecido para outliers; en caso afirmativo lo imprime por pantalla. En la salida anterior se puede observar que no hay ningún outlier, ya que ningún residuo supera el umbral establecido.\\ 
	 
	 Se va a probar con otro conjunto de datos para demostrar que por medio de este método podemos identificar las anomalías. En primer lugar, se vuelve a confeccionar un archivo de texto (\texttt{.txt}) atendiendo a las normas previamente expuestas. Este archivo de texto se llamará \texttt{planetas2.txt} para evitar problemas de sobrescritura. 
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(muestra} \hlkwb{=} \hlkwd{read.table}\hlstd{(}\hlstr{"data/planetas2.txt"}\hlstd{))}
\end{alltt}
\begin{verbatim}
##       R    D
## 1.  3.0  2.0
## 2.  3.5 12.0
## 3.  4.7  4.1
## 4.  5.2  4.9
## 5.  7.1  6.1
## 6.  6.2  5.2
## 7. 14.0  5.3
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Una vez hecho esto se invoca a la función \texttt{lm} para sacar la regresión y se guarda el vector de residuos tal y como se ha hecho previamente.
	 
	 {\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(dfr}\hlkwb{=}\hlkwd{lm}\hlstd{(D}\hlopt{~}\hlstd{R,} \hlkwc{data}\hlstd{=muestra))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = D ~ R, data = muestra)
## 
## Coefficients:
## (Intercept)            R  
##     6.01445     -0.05723
\end{verbatim}
\begin{alltt}
\hlstd{(res}\hlkwb{=}\hlkwd{summary}\hlstd{(dfr)}\hlopt{$}\hlstd{residuals)}
\end{alltt}
\begin{verbatim}
##         1.         2.         3.         4.         5.         6.         7. 
## -3.8427477  6.1858698 -1.6454482 -0.8168308  0.4919157 -0.4595958  0.0868370
\end{verbatim}
\end{kframe}
\end{knitrout}
	}
	 Se vuelve a calcular el error estándar de los residuos. Atendiendo a la fórmula hay que dividir entre 7, ya que se tienen 7 entradas en el conjunto de datos del fichero. 
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(sr} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(res}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlnum{7}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 2.850242
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Una vez se tienen todos los datos necesarios, se pone a prueba el código visto previamente para detectar anomalías. Esta vez se cambiará el grado de outlier $d = 2$.
	 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(res))\{}
        \hlkwa{if}\hlstd{(res[i]}\hlopt{>}\hlnum{2}\hlopt{*}\hlstd{sr)\{}
                \hlkwd{print}\hlstd{(}\hlstr{"el suceso"}\hlstd{);}
                \hlkwd{print}\hlstd{(res[i]);}
                \hlkwd{print}\hlstd{(}\hlstr{"es un suceso anómalo o outlier"}\hlstd{);}
                 \hlstd{\}}
\hlstd{\}}
\end{alltt}
\begin{verbatim}
## [1] "el suceso"
##      2. 
## 6.18587 
## [1] "es un suceso anómalo o outlier"
\end{verbatim}
\end{kframe}
\end{knitrout}
	 
	 Se observa en la salida del código que el suceso 2 es un outlier. Atendiendo a los parámetros que se han ido obteniendo, se observa a simple vista que el suceso 2 supera el límite establecido. 
\end{document}
